# BML-project
Stochastic Gradient Langevin Dynamics
Nowadays, most of the problems, that Machine Learning has to solve, are based principally on very large scale datasets. Therefore, more and more advances are required in order to meet such needs.
In this context, the paper titled \textbf{Bayesian Learning via Stochastic Gradient LangevinDynamics} came along with a new method for learning from this kind of datasets based on iterative learning from small mini-batches, by combining the usual stochastic gradient optimization algorithm and Bayesian posterior sampling.
Hence, the objective of this github project is to implement the method, then analyze it and apply it to a real data so as to have a concrete results of the efficiency and the performance of the method.

We first implement the algorithm by ourselves using python and then apply it on three different bayesian learnin problems:  the \textbf{Logistic regression} withthe a9a dataset, then . Then, using a \textbf{Gaussian Mixture} dataset. And finally, using the Iris dataset.
